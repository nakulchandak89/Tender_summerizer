{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11016372,"sourceType":"datasetVersion","datasetId":6859219},{"sourceId":11017541,"sourceType":"datasetVersion","datasetId":6860035}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nltk PyPDF2 transformers torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:20:02.965642Z","iopub.status.idle":"2025-03-13T08:20:02.965976Z","shell.execute_reply":"2025-03-13T08:20:02.965849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport nltk\nimport PyPDF2\nimport torch\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Download necessary NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:20:22.625086Z","iopub.execute_input":"2025-03-13T08:20:22.625463Z","iopub.status.idle":"2025-03-13T08:20:22.636207Z","shell.execute_reply.started":"2025-03-13T08:20:22.625431Z","shell.execute_reply":"2025-03-13T08:20:22.635158Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')  # Required for WordNet\nnltk.download('punkt')    # For tokenization\nnltk.download('stopwords')  # For stopwords\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:20:23.067084Z","iopub.execute_input":"2025-03-13T08:20:23.067479Z","iopub.status.idle":"2025-03-13T08:20:23.136683Z","shell.execute_reply.started":"2025-03-13T08:20:23.067447Z","shell.execute_reply":"2025-03-13T08:20:23.135595Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Load NLP Model (T5 for text generation)\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n# Predefined stopwords\nsw = set(stopwords.words('english'))\n\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"Extract text from a local PDF file.\"\"\"\n    text = \"\"\n    with open(pdf_path, \"rb\") as file:\n        pdf_reader = PyPDF2.PdfReader(file)\n        for page in pdf_reader.pages:\n            text += page.extract_text() + \" \"\n    return text.strip()\n\ndef clean_text(text):\n    \"\"\"Clean text by removing special characters and stopwords.\"\"\"\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n    text = text.lower()  # Convert to lowercase\n    words = word_tokenize(text)  # Tokenize words\n    clean_words = [word for word in words if word not in sw]  # Remove stopwords\n    return ' '.join(clean_words)\n\n\ndef generate_text(cleaned_text):\n    \"\"\"Generate new text using T5 NLP model.\"\"\"\n    input_text = \"summarize: \" + cleaned_text  # T5 works well with \"summarize:\" prefix\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n    summary_ids = model.generate(inputs.input_ids, max_length=150, num_beams=4, early_stopping=True)\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:52:36.704587Z","iopub.execute_input":"2025-03-13T08:52:36.704962Z","iopub.status.idle":"2025-03-13T08:52:37.796250Z","shell.execute_reply.started":"2025-03-13T08:52:36.704932Z","shell.execute_reply":"2025-03-13T08:52:37.795170Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Set stopwords\nsw = set(stopwords.words('english'))\n\ndef clean_text(text):\n    \"\"\"Clean text by removing special characters and stopwords.\"\"\"\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n    text = text.lower()  # Convert to lowercase\n    words = word_tokenize(text)  # Tokenize words\n    clean_words = [word for word in words if word not in sw]  # Remove stopwords\n    return ' '.join(clean_words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:52:37.899940Z","iopub.execute_input":"2025-03-13T08:52:37.900266Z","iopub.status.idle":"2025-03-13T08:52:37.909533Z","shell.execute_reply.started":"2025-03-13T08:52:37.900240Z","shell.execute_reply":"2025-03-13T08:52:37.908386Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"pdf_path = \"/kaggle/input/sample2/Tender summarization sumarization common festurees.pdf\"  # Update this with the correct file path\n\n# Step 1: Extract Text\nraw_text = extract_text_from_pdf(pdf_path)\nprint(\"Extracted Text:\", raw_text[:4000])  # Print first 500 characters\n\n# Step 2: Clean Text\ncleaned_text = clean_text(raw_text)\nprint(\"Cleaned Text:\", cleaned_text[:4000])  # Print first 500 characters\n\n# Step 3: Generate Summary/Text\ngenerated_text = generate_text(cleaned_text)\nprint(\"Generated Text:\", generated_text)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}